{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    ".tf  {border-collapse:collapse;border-spacing:0;width:100%}\n",
    ".tf td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  overflow:hidden;padding:10px 5px;w-break:normal;}\n",
    ".tf th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  font-weight:normal;overflow:hidden;padding:10px 5px;w-break:normal;}\n",
    ".tf .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}\n",
    ".col1 { width: 20%;}\n",
    ".col2 { width: 80%;}\n",
    "</style>\n",
    "<table class=\"tf\">\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky col1\">Name</th>\n",
    "    <th class=\"tg-0pky col2\">Pranay Singhvi</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky col1\">UID No.</td>\n",
    "    <td class=\"tg-0pky col2\">2021300126</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky col1\">Experiment No.</td>\n",
    "    <td class=\"tg-0pky col2\">5</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <p style=\"text-align:center;font-weight:500;\">Experiment 5</p>\n",
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;width:100%}\n",
    ".tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  overflow:hidden;padding:10px 5px;w-break:normal;}\n",
    ".tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  font-weight:normal;overflow:hidden;padding:10px 10px;w-break:normal;}\n",
    ".tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top;}\n",
    ".col1 { width: 20%;}\n",
    ".col2 { width: 80%;}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky col1\">Aim</th>\n",
    "    <th class=\"tg-0pky col2\">To calculate emission and transition matrix for tagging Parts of Speech using Hidden\n",
    "Markov Model. Find POS tag of given sentence using HMM.</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky col1\">Theory</th>\n",
    "    <th class=\"tg-0pky col2\">\n",
    "    <p>Hidden Markov Model (HMM) is a statistical Markov model in which the system being modeled is assumed to be a Markov process with unobserved (i.e. hidden) states. HMM is a doubly stochastic process, where the observed data is assumed to be generated by a stochastic process. The model is defined by the following components:</p>\n",
    "    <p>1. A set of N states, where N is the number of states in the model.</p>\n",
    "    <p>2. A set of M observation symbols, where M is the number of distinct observation symbols per state.</p>\n",
    "    <p>3. A state transition probability matrix A, where A[i][j] is the probability of transitioning from state i to state j.</p>\n",
    "    <p>4. An observation probability matrix B, where B[i][j] is the probability of observing symbol j from state i.</p>\n",
    "    <p>5. An initial state distribution π, where π[i] is the probability of starting in state i.</p>\n",
    "    <p>Given a sequence of observations, the model aims to find the most likely sequence of states that generated the observations. This is done using the Viterbi algorithm, which is a dynamic programming algorithm for finding the most likely sequence of hidden states.</p>\n",
    "  </th>\n",
    "  </tr>\n",
    "</thead>\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Installation of NLTK and downloading the required corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from prettytable import PrettyTable\n",
    "from collections import defaultdict\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Loading the corpus and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 47959 sentences in the dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Word  POS\n",
       "0      Thousands  NNS\n",
       "1             of   IN\n",
       "2  demonstrators  NNS\n",
       "3           have  VBP\n",
       "4        marched  VBN"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load csv\n",
    "df = pd.read_csv('exp5.csv', encoding='isO-8859-1')\n",
    "df1 = df[df['Sentence #'].notna()]\n",
    "print(\"There are\",df1['Sentence #'].iloc[-1].split()[-1],\"sentences in the dataset\")\n",
    "df.drop(['Sentence #', 'Tag'], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in POS column: ['NNS' 'IN' 'VBP' 'VBN' 'NNP' 'TO' 'VB' 'DT' 'NN' 'CC' 'JJ' '.' 'VBD' 'WP'\n",
      " '``' 'CD' 'PRP' 'VBZ' 'POS' 'VBG' 'RB' ',' 'WRB' 'PRP$' 'MD' 'WDT' 'JJR'\n",
      " ':' 'JJS' 'WP$' 'RP' 'PDT' 'NNPS' 'EX' 'RBS' 'LRB' 'RRB' '$' 'RBR' ';'\n",
      " 'UH' 'FW']\n"
     ]
    }
   ],
   "source": [
    "# print all unique values in POS column\n",
    "print(\"Unique values in POS column:\",df['POS'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # remove punctuation\n",
    "    text = text.replace(\"\\n\", \" \") # remove \\n\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove non-w characters\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra whitespaces\n",
    "    text = re.sub(r'\\d', '', text)  # Remove digits\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_mapping = {\n",
    "    'NN': 'NOUN',\n",
    "    'NNS': 'NOUN',\n",
    "    'NNP': 'NOUN',\n",
    "    'NNPS': 'NOUN',\n",
    "    'VB': 'VERB',\n",
    "    'VBD': 'VERB',\n",
    "    'VBG': 'VERB',\n",
    "    'VBN': 'VERB',\n",
    "    'VBP': 'VERB',\n",
    "    'VBZ': 'VERB',\n",
    "    'JJ': 'ADJ',\n",
    "    'JJR': 'ADJ',\n",
    "    'JJS': 'ADJ',\n",
    "    'RB': 'ADV',\n",
    "    'RBR': 'ADV',\n",
    "    'RBS': 'ADV',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Building Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the dataframe to a dictionary, make value field as list of all the tags of that w in the sentence\n",
    "vocab = {}\n",
    "for index, row in df.iterrows():\n",
    "    w = row['Word']\n",
    "    pos = row['POS']\n",
    "    tag = tag_mapping.get(row['POS'], 'MODAL')\n",
    "    # if only string\n",
    "    if type(w) == str:\n",
    "        if w == ';' or w == ':' or w == '``' or w == ',' or w == '.':\n",
    "            continue\n",
    "        else:\n",
    "            w = preprocess(w)\n",
    "    else:\n",
    "        w = str(w)\n",
    "        continue\n",
    "    w = preprocess(w)\n",
    "    if w in vocab and tag not in vocab[w]:\n",
    "        vocab[w].append(tag)\n",
    "    else:\n",
    "        if w not in vocab:\n",
    "            vocab[w] = [tag]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Calculating Emission & Transition Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "emission_matrix = defaultdict(lambda: defaultdict(int))\n",
    "    # calculate the emission probability and store it in the emission matrix\n",
    "for index, row in df.iterrows():\n",
    "    w = row['Word']\n",
    "    tag = tag_mapping.get(row['POS'], 'MODAL')\n",
    "    if type(w) == str:\n",
    "        w = preprocess(w)\n",
    "    else:\n",
    "        w = str(w)\n",
    "        continue\n",
    "    w = preprocess(w)\n",
    "    emission_matrix[w][tag] += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emission Matrix:\n",
      "+---------------+------+-----+------+-----+-------+\n",
      "|               | VERB | ADV | NOUN | ADJ | MODAL |\n",
      "+---------------+------+-----+------+-----+-------+\n",
      "|   thousands   | 0.0  | 0.0 | 1.0  | 0.0 |  0.0  |\n",
      "|       of      | 0.0  | 0.0 | 0.0  | 0.0 |  1.0  |\n",
      "| demonstrators | 0.0  | 0.0 | 1.0  | 0.0 |  0.0  |\n",
      "|      have     | 1.0  | 0.0 | 0.0  | 0.0 |  0.0  |\n",
      "|    marched    | 1.0  | 0.0 | 0.0  | 0.0 |  0.0  |\n",
      "|    through    | 0.0  | 0.0 | 0.0  | 0.0 |  1.0  |\n",
      "|     london    | 0.0  | 0.0 | 1.0  | 0.0 |  0.0  |\n",
      "|       to      | 0.0  | 0.0 | 0.0  | 0.0 |  1.0  |\n",
      "|    protest    | 0.48 | 0.0 | 0.52 | 0.0 |  0.0  |\n",
      "|      the      | 0.0  | 0.0 | 0.0  | 0.0 |  1.0  |\n",
      "+---------------+------+-----+------+-----+-------+\n"
     ]
    }
   ],
   "source": [
    "emi_tab = PrettyTable()\n",
    "emi_tab.field_names = [\"\"] + list(set(tag_mapping.values())) + ['MODAL']\n",
    "for w in emission_matrix:\n",
    "    total = sum(emission_matrix[w].values())\n",
    "    prob = {tag: round(emission_matrix[w][tag] / total, 2) for tag in emi_tab.field_names[1:]}\n",
    "    emi_tab.add_row([w] + list(prob.values()))\n",
    "print(\"Emission Matrix:\")\n",
    "# print only first 10 rows\n",
    "print(emi_tab[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_mat = defaultdict(lambda: defaultdict(int))\n",
    "prev_tag = None\n",
    "for index, row in df.iterrows():\n",
    "    tag = tag_mapping.get(row['POS'], 'MODAL')\n",
    "    if prev_tag is not None:\n",
    "        trans_mat[prev_tag][tag] += 1\n",
    "    prev_tag = tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transition Matrix:\n",
      "+-------+------+------+------+------+-------+\n",
      "|       | VERB | ADV  | NOUN | ADJ  | MODAL |\n",
      "+-------+------+------+------+------+-------+\n",
      "|  NOUN | 0.55 | 0.18 | 0.01 | 0.25 |  0.01 |\n",
      "| MODAL | 0.31 | 0.13 | 0.02 | 0.41 |  0.14 |\n",
      "|  VERB | 0.54 | 0.18 | 0.05 | 0.16 |  0.07 |\n",
      "|  ADJ  | 0.13 | 0.01 | 0.0  | 0.77 |  0.09 |\n",
      "|  ADV  | 0.39 | 0.41 | 0.05 | 0.04 |  0.1  |\n",
      "+-------+------+------+------+------+-------+\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTransition Matrix:\")\n",
    "trans_table = PrettyTable()\n",
    "trans_table.field_names = [\"\"] + list(set(tag_mapping.values())) + ['MODAL']\n",
    "for tag in trans_mat:\n",
    "    total = sum(trans_mat[tag].values())\n",
    "    prob = {}\n",
    "    for tg in set(tag_mapping.values()) | {'MODAL'}:\n",
    "        prob[tg] = round(trans_mat[tag][tg] / total, 2)\n",
    "    trans_table.add_row([tag] + list(prob.values()))\n",
    "print(trans_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Predicting POS tags for a given sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform POS tagging\n",
    "def predict_pos(sentence):\n",
    "    pre_tags = []\n",
    "    for w in sentence.split():\n",
    "        w = preprocess(w)\n",
    "        if w in vocab:\n",
    "            pre_tag = max(vocab[w], key=lambda tag: emission_matrix[w][tag] if tag in emission_matrix[w] else 0)\n",
    "        else:\n",
    "            pre_tag = 'UNK'  # If w not in vocabulary, assign 'UNK' tag\n",
    "        pre_tags.append(pre_tag)\n",
    "    return pre_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted POS Tags:\n",
      "['MODAL', 'NOUN', 'VERB', 'MODAL', 'MODAL', 'NOUN', 'VERB', 'MODAL', 'ADJ', 'ADJ', 'UNK', 'MODAL', 'MODAL', 'UNK', 'UNK', 'NOUN', 'MODAL', 'MODAL', 'NOUN']\n"
     ]
    }
   ],
   "source": [
    "sample_sentence = \"The sun dipped below the horizon, casting a warm, golden glow across the tranquil, rippling waters of the lake.\"\n",
    "pre_tags = predict_pos(sample_sentence)\n",
    "print(\"\\nPredicted POS Tags:\")\n",
    "print(pre_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Curiosity Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <p style=\"font-weight:500; \">Q1. List a few ways for tagging parts of speech?</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: There are several ways to tag parts of speech. Some of them are:\n",
    "\n",
    "1. **Rule-Based Tagging:**\n",
    "   - **Theory:** Rule-based tagging involves creating a set of linguistic rules and patterns to determine the part of speech for each word in a sentence. These rules are based on grammatical structures and word patterns.\n",
    "   - **Example:** If a word ends with \"-ing,\" it is likely a gerund (verb form used as a noun).\n",
    "\n",
    "2. **Stochastic Tagging:**\n",
    "   - **Theory:** Stochastic tagging involves using statistical models to assign probabilities to different parts of speech for a given word. These models learn from training data to estimate the likelihood of a word belonging to a specific part of speech.\n",
    "   - **Example:** Hidden Markov Models and Maximum Entropy Models are examples of stochastic tagging methods.\n",
    "\n",
    "3. **Transformation-Based Tagging:**\n",
    "   - **Theory:** Transformation-based tagging is a type of rule-based tagging where rules are learned from training data through a process of error-driven transformation. The system starts with an initial tagging and improves it iteratively.\n",
    "   - **Example:** Brill Tagger is a transformation-based tagging algorithm.\n",
    "\n",
    "4. **Hidden Markov Models (HMM):**\n",
    "   - **Theory:** HMMs are probabilistic models that represent a sequence of observable events (words) and a sequence of hidden states (parts of speech). The model estimates the probability of transitioning between states and emitting observations.\n",
    "   - **Example:** In POS tagging, the hidden states are parts of speech, and the observations are words.\n",
    "\n",
    "5. **Maximum Entropy Models:**\n",
    "   - **Theory:** Maximum Entropy Models aim to find the probability distribution that maximizes entropy (uncertainty) given a set of constraints. In POS tagging, these models learn the most likely part-of-speech tags based on observed features.\n",
    "   - **Example:** Conditional Maximum Entropy Models are commonly used for part-of-speech tagging tasks.\n",
    "\n",
    "6. **Deep Learning Models:**\n",
    "   - **Theory:** Deep learning models, such as recurrent neural networks (RNNs) and long short-term memory networks (LSTMs), use neural networks with multiple layers to capture complex relationships and dependencies in sequential data like language.\n",
    "   - **Example:** Bidirectional LSTMs are effective for part-of-speech tagging as they consider both past and future context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <p style=\"font-weight:500;\">Q2. How do you find the most probable sequence of POS tags from a sequence of text?</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Ans: Hidden Markov Models (HMMs) and the Viterbi algorithm are applied in Part-of-Speech (POS) tagging to find the most probable sequence of POS tags from a given text. HMMs model the probabilistic relationships between POS tags and observed words. Trained on labeled datasets, HMMs estimate transition probabilities between tags and emission probabilities for observed words. The Viterbi algorithm efficiently determines the most likely sequence of hidden states (POS tags) given a sequence of observations (words). It involves initialization, recursion, backtracking, and termination steps. In POS tagging, each word in a sentence corresponds to an observation, and the Viterbi algorithm calculates the optimal sequence of POS tags by considering both transition and emission probabilities. This approach efficiently narrows down the search space, making it computationally feasible for large datasets. Ultimately, the combination of HMMs and the Viterbi algorithm provides a probabilistic framework for accurate and efficient POS tagging in natural language processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <p style=\"font-weight:500;\">Q3. Differentiate between Markov chain and Markov model?</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Ans: A Markov chain and a Markov model are related concepts in probability theory and stochastic processes, but they have distinct characteristics:\n",
    "\n",
    "**Markov Chain:**\n",
    "1. **Definition:** A Markov chain is a mathematical model that describes a sequence of events where the probability of transitioning to any particular state depends solely on the current state and time elapsed, and not on the sequence of events leading to that state.\n",
    "2. **Memoryless Property:** Markov chains possess the Markov property, indicating that the future state depends only on the current state and is independent of the past states.\n",
    "3. **States and Transitions:** Markov chains consist of a set of states and probabilities of transitioning between those states.\n",
    "4. **Example:** Consider a board game where the probability of moving to the next square depends only on the current square and not on how the game reached that point.\n",
    "\n",
    "**Markov Model:**\n",
    "1. **Definition:** A Markov model is a broader term that encompasses various mathematical models, including Markov chains. It is a general framework for modeling systems where a process evolves over time through a series of states, and transitions between states are probabilistic.\n",
    "2. **Incorporating More Complexity:** While Markov chains specifically refer to a simple memoryless process, Markov models can include additional complexity, such as different types of states, continuous-time transitions, or multiple interacting processes.\n",
    "3. **Application:** Markov models find applications in various fields, including economics, biology, and computer science, allowing for more flexibility and customization to specific scenarios.\n",
    "\n",
    "In summary, a Markov chain is a specific type of Markov model that represents a simple, memoryless process with probabilistic state transitions. Markov models, in a broader sense, encompass a range of models that can incorporate additional features and complexities beyond the basic Markov chain structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <p style=\"font-weight:500;\">Q4. How you can identify whether a system follows a Markov Process?</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: To identify if a system follows a Markov process:\n",
    "\n",
    "1. **Markov Property:**\n",
    "   - Check if future behavior depends only on the current state, independent of past states.\n",
    "\n",
    "2. **Memoryless Transitions:**\n",
    "   - Confirm that transitioning to the next state depends solely on the current state, with no influence from past events.\n",
    "\n",
    "3. **Transition Probabilities:**\n",
    "   - Ensure that transition probabilities are fixed and not influenced by previous states.\n",
    "\n",
    "4. **State Space:**\n",
    "   - Define a discrete set of possible states for straightforward application of the Markov property.\n",
    "\n",
    "5. **Conditional Independence:**\n",
    "   - Verify that the probability of the next state, given the current state, is conditionally independent of past states.\n",
    "\n",
    "6. **Stationary Transition Probabilities (Optional):**\n",
    "   - Check if transition probabilities remain constant over time.\n",
    "\n",
    "7. **Transition Diagrams/Matrices:**\n",
    "   - Use visual aids like transition diagrams or matrices to represent and analyze state transitions.\n",
    "\n",
    "If these conditions hold, the system can be considered a Markov process. Memorylessness and independence of past events in state transitions are key indicators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <p style=\"font-weight:500;\">Q5.  Explain the use of Markov Chains in text generation algorithms.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: Markov Chains are widely employed in text generation algorithms to model and generate sequences of words or characters based on the probability of transitioning from one state to another. Here's how they are used:\n",
    "\n",
    "1. **Modeling Sequential Dependencies:**\n",
    "   - Markov Chains capture sequential dependencies in a text corpus. Each state represents a word or a sequence of words, and transitions between states are determined by the likelihood of a word following another.\n",
    "\n",
    "2. **Order of the Markov Chain:**\n",
    "   - The \"order\" of a Markov Chain determines the number of previous states considered when predicting the next state. For text generation, a higher-order Markov Chain incorporates more context, providing a more sophisticated understanding of language structure.\n",
    "\n",
    "3. **Transition Probabilities:**\n",
    "   - Transition probabilities are estimated from a training dataset, representing how often specific sequences of words occur. These probabilities guide the generation process, determining the likelihood of transitioning to different words.\n",
    "\n",
    "4. **Text Generation Process:**\n",
    "   - To generate text, start with an initial state (word or sequence), and iteratively choose the next state based on the transition probabilities. This process is repeated until the desired length of text is generated.\n",
    "\n",
    "5. **N-gram Models:**\n",
    "   - Markov Chains are related to N-gram models, where the order of the Markov Chain corresponds to the size of the N-gram. For example, a bigram model is a second-order Markov Chain.\n",
    "\n",
    "6. **Applications:**\n",
    "   - Markov Chains find applications in various text generation tasks, including auto-completion suggestions, chatbot responses, and even creative writing or poetry generation.\n",
    "\n",
    "7. **Limitations:**\n",
    "   - While Markov Chains capture local dependencies well, they may struggle with long-range dependencies in language. Higher-order Markov Chains mitigate this to some extent but may face data sparsity issues.\n",
    "\n",
    "8. **Adaptations:**\n",
    "   - Some text generation algorithms combine Markov Chains with other techniques, such as neural networks, to capture more complex patterns and semantic understanding.\n",
    "\n",
    "In essence, Markov Chains provide a simple yet effective way to model the probabilistic nature of language and generate coherent text based on learned sequential dependencies from training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Conclusion\n",
    "\n",
    "In this experiment, we explored Hidden Markov Models (HMMs) for Part-of-Speech (POS) tagging. We implemented an HMM, calculated emission and transition matrices, and applied the model to identify POS tags in sentences. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bap-notebook",
   "language": "python",
   "name": "bap-notebook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
