{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    ".tf  {border-collapse:collapse;border-spacing:0;width:100%}\n",
    ".tf td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  overflow:hidden;padding:10px 5px;w-break:normal;}\n",
    ".tf th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  font-weight:normal;overflow:hidden;padding:10px 5px;w-break:normal;}\n",
    ".tf .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}\n",
    ".col1 { width: 20%;}\n",
    ".col2 { width: 80%;}\n",
    "</style>\n",
    "<table class=\"tf\">\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky col1\">Name</th>\n",
    "    <th class=\"tg-0pky col2\">Pranay Singhvi</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky col1\">UID No.</td>\n",
    "    <td class=\"tg-0pky col2\">2021300126</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky col1\">Experiment No.</td>\n",
    "    <td class=\"tg-0pky col2\">6</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <p style=\"text-align:center;font-weight:500;\">Experiment 6</p>\n",
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;width:100%}\n",
    ".tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  overflow:hidden;padding:10px 5px;w-break:normal;}\n",
    ".tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  font-weight:normal;overflow:hidden;padding:10px 10px;w-break:normal;}\n",
    ".tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top;}\n",
    ".col1 { width: 20%;}\n",
    ".col2 { width: 80%;}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky col1\">Aim</th>\n",
    "    <th class=\"tg-0pky col2\">To calculate emission and transition matrix and find Find POS tags of ws in a sentence using Viterbi decoding</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky col1\">Theory</th>\n",
    "    <th class=\"tg-0pky col2\">In natural language processing, part-of-speech (POS) tagging is the process of assigning a grammatical category (such as noun, verb, adjective, etc.) to each word in a sentence. This is an important task in many NLP applications, as it helps in understanding the structure and meaning of text.\n",
    "\n",
    "### 1. Hidden Markov Models (HMM):\n",
    "Hidden Markov Models are statistical models used for modeling systems that transition between different states over time. In the context of natural language processing, HMMs are often used to model sequences of words and their associated POS tags.\n",
    "\n",
    "### 2. Emission Matrix:\n",
    "The emission matrix represents the probabilities of observing a certain word given a particular POS tag. Each row corresponds to a POS tag, and each column corresponds to a word. The cell (i, j) contains the probability of the word j being emitted by the POS tag i.\n",
    "$$\n",
    "\\text{Emission Matrix}[i, j] = P(\\text{word } j \\,|\\, \\text{POS tag } i) \n",
    "$$\n",
    "### 3. Transition Matrix:\n",
    "The transition matrix represents the probabilities of transitioning from one POS tag to another. Each row corresponds to a current POS tag, and each column corresponds to a possible next POS tag. The cell (i, j) contains the probability of transitioning from POS tag i to POS tag j.\n",
    "$$\n",
    " \\text{Transition Matrix}[i, j] = P(\\text{POS tag } j \\,|\\, \\text{POS tag } i)\n",
    "$$\n",
    "### 4. Viterbi Decoding:\n",
    "Viterbi decoding is a dynamic programming algorithm used to find the most likely sequence of hidden states in a Hidden Markov Model. In the context of POS tagging, Viterbi decoding helps find the most probable sequence of POS tags for a given sequence of words.\n",
    "\n",
    "The algorithm involves calculating probabilities recursively and keeping track of the most probable path at each step. The steps are as follows:\n",
    "\n",
    "- **Initialization:** Initialize the probability matrix for the first word using emission probabilities.\n",
    "- **Recursion:** For each subsequent word, calculate the probability of reaching each state from the previous states by considering both emission and transition probabilities.\n",
    "- **Backtracking:** Keep track of the most likely path at each step.\n",
    "- **Termination:** The final state in the sequence is the one with the highest probability.\n",
    "\n",
    "### 5. Application to POS Tagging:\n",
    "Given a sentence, you can use the emission and transition matrices to apply Viterbi decoding and determine the most likely sequence of POS tags for each word in the sentence.\n",
    "$$\n",
    "\\text{POS Tags} = \\text{Viterbi Decoding}(\\text{Emission Matrix}, \\text{Transition Matrix}, \\text{Sentence}) \n",
    "$$\n",
    "This process helps in automatically assigning POS tags to words in a sentence based on statistical probabilities derived from training data.\n",
    "\n",
    "\n",
    "### Viterbi Algorithm:\n",
    "\n",
    "1. **Initialization:**\n",
    "   - Initialize the probability matrix, `V`, with dimensions `[num_states, num_words]`.\n",
    "   - Set the initial probabilities in the first column based on the product of the initial probabilities of states and emission probabilities of the first word.\n",
    "\n",
    "2. **Recursion:**\n",
    "   - For each subsequent word in the sentence:\n",
    "     - Calculate the probability of transitioning from the previous state to the current state, multiplied by the emission probability of the current word.\n",
    "     - Update the probability matrix with the maximum probability and the corresponding backpointer.\n",
    "$$\n",
    " V[i, j] = \\max(V[k, j-1] \\times \\text{transition}[k, i] \\times \\text{emission}[i, \\text{word}_j]), \\, \\text{for all } i \n",
    "$$\n",
    "3. **Backtracking:**\n",
    "   - Start from the final state with the highest probability in the last column.\n",
    "   - Follow the backpointers to trace back the most likely sequence of states.\n",
    "\n",
    "4. **Termination:**\n",
    "   - The final sequence of POS tags is obtained by backtracking from the state with the highest probability in the last column.\n",
    "### Explanation:\n",
    "\n",
    "The Viterbi algorithm is based on dynamic programming principles. It efficiently computes the probability of the most likely sequence of states by breaking down the problem into smaller subproblems and reusing the solutions to these subproblems. The algorithm maintains a matrix of probabilities and backpointers, allowing it to track the most probable path at each step.\n",
    "\n",
    "By considering both the transition probabilities between states and the emission probabilities for observing specific words, Viterbi decoding finds the sequence of hidden states (POS tags) that maximizes the overall probability of the observed sequence of words. The backtracking step then allows us to reconstruct this most likely sequence.\n",
    "\n",
    "It's a fundamental algorithm used in various applications, including POS tagging, speech recognition, and bioinformatics. Its efficiency and optimality make it a valuable tool for finding the most probable sequence of hidden states in a Hidden Markov Model.\n",
    "\n",
    "### Advantages of Viterbi Algorithm:\n",
    "\n",
    "1. **Efficiency:** The Viterbi algorithm is computationally efficient, with a time complexity that is linear with respect to the length of the sequence.\n",
    "\n",
    "2. **Optimality:** It guarantees the optimal solution, meaning that the sequence it produces has the maximum probability given the model parameters.\n",
    "\n",
    "### Disadvantages of Viterbi Algorithm:\n",
    "\n",
    "1. **Assumption of Independence:** Viterbi assumes that the observations (words) are conditionally independent given the hidden states. This may not always hold true in real-world language usage.\n",
    "\n",
    "2. **Sensitivity to Model Parameters:** The performance of Viterbi decoding heavily depends on the accuracy of the emission and transition probabilities derived from the training data. Inaccuracies in these parameters can lead to suboptimal results.\n",
    "\n",
    "</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Installation of NLTK and downloading the required corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from prettytable import PrettyTable\n",
    "from collections import defaultdict\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Loading the corpus and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 47959 sentences in the dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Word  POS\n",
       "0      Thousands  NNS\n",
       "1             of   IN\n",
       "2  demonstrators  NNS\n",
       "3           have  VBP\n",
       "4        marched  VBN"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load csv\n",
    "df = pd.read_csv('exp5.csv', encoding='isO-8859-1')\n",
    "df1 = df[df['Sentence #'].notna()]\n",
    "print(\"There are\",df1['Sentence #'].iloc[-1].split()[-1],\"sentences in the dataset\")\n",
    "df.drop(['Sentence #', 'Tag'], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in POS column: ['NNS' 'IN' 'VBP' 'VBN' 'NNP' 'TO' 'VB' 'DT' 'NN' 'CC' 'JJ' '.' 'VBD' 'WP'\n",
      " '``' 'CD' 'PRP' 'VBZ' 'POS' 'VBG' 'RB' ',' 'WRB' 'PRP$' 'MD' 'WDT' 'JJR'\n",
      " ':' 'JJS' 'WP$' 'RP' 'PDT' 'NNPS' 'EX' 'RBS' 'LRB' 'RRB' '$' 'RBR' ';'\n",
      " 'UH' 'FW']\n"
     ]
    }
   ],
   "source": [
    "# print all unique values in POS column\n",
    "print(\"Unique values in POS column:\",df['POS'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # remove punctuation\n",
    "    text = text.replace(\"\\n\", \" \") # remove \\n\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove non-w characters\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra whitespaces\n",
    "    text = re.sub(r'\\d', '', text)  # Remove digits\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_mapping = {\n",
    "    'NN': 'NOUN',\n",
    "    'NNS': 'NOUN',\n",
    "    'NNP': 'NOUN',\n",
    "    'NNPS': 'NOUN',\n",
    "    'VB': 'VERB',\n",
    "    'VBD': 'VERB',\n",
    "    'VBG': 'VERB',\n",
    "    'VBN': 'VERB',\n",
    "    'VBP': 'VERB',\n",
    "    'VBZ': 'VERB',\n",
    "    'JJ': 'ADJ',\n",
    "    'JJR': 'ADJ',\n",
    "    'JJS': 'ADJ',\n",
    "    'RB': 'ADV',\n",
    "    'RBR': 'ADV',\n",
    "    'RBS': 'ADV',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Building Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the dataframe to a dictionary, make value field as list of all the tags of that w in the sentence\n",
    "vocab = {}\n",
    "for index, row in df.iterrows():\n",
    "    w = row['Word']\n",
    "    pos = row['POS']\n",
    "    tag = tag_mapping.get(row['POS'], 'MODAL')\n",
    "    # if only string\n",
    "    if type(w) == str:\n",
    "        if w == ';' or w == ':' or w == '``' or w == ',' or w == '.':\n",
    "            continue\n",
    "        else:\n",
    "            w = preprocess(w)\n",
    "    else:\n",
    "        w = str(w)\n",
    "        continue\n",
    "    w = preprocess(w)\n",
    "    if w in vocab and tag not in vocab[w]:\n",
    "        vocab[w].append(tag)\n",
    "    else:\n",
    "        if w not in vocab:\n",
    "            vocab[w] = [tag]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Calculating Emission & Transition Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "emission_matrix = defaultdict(lambda: defaultdict(int))\n",
    "    # calculate the emission probability and store it in the emission matrix\n",
    "for index, row in df.iterrows():\n",
    "    w = row['Word']\n",
    "    tag = tag_mapping.get(row['POS'], 'MODAL')\n",
    "    if type(w) == str:\n",
    "        w = preprocess(w)\n",
    "    else:\n",
    "        w = str(w)\n",
    "        continue\n",
    "    w = preprocess(w)\n",
    "    emission_matrix[w][tag] += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emission Matrix:\n",
      "+---------------+-----+------+------+-----+-------+\n",
      "|               | ADV | VERB | NOUN | ADJ | MODAL |\n",
      "+---------------+-----+------+------+-----+-------+\n",
      "|   thousands   | 0.0 | 0.0  | 1.0  | 0.0 |  0.0  |\n",
      "|       of      | 0.0 | 0.0  | 0.0  | 0.0 |  1.0  |\n",
      "| demonstrators | 0.0 | 0.0  | 1.0  | 0.0 |  0.0  |\n",
      "|      have     | 0.0 | 1.0  | 0.0  | 0.0 |  0.0  |\n",
      "|    marched    | 0.0 | 1.0  | 0.0  | 0.0 |  0.0  |\n",
      "|    through    | 0.0 | 0.0  | 0.0  | 0.0 |  1.0  |\n",
      "|     london    | 0.0 | 0.0  | 1.0  | 0.0 |  0.0  |\n",
      "|       to      | 0.0 | 0.0  | 0.0  | 0.0 |  1.0  |\n",
      "|    protest    | 0.0 | 0.48 | 0.52 | 0.0 |  0.0  |\n",
      "|      the      | 0.0 | 0.0  | 0.0  | 0.0 |  1.0  |\n",
      "+---------------+-----+------+------+-----+-------+\n"
     ]
    }
   ],
   "source": [
    "emission_table = PrettyTable()\n",
    "emission_table.field_names = [\"\"] + list(set(tag_mapping.values())) + ['MODAL']\n",
    "for w in emission_matrix:\n",
    "    total = sum(emission_matrix[w].values())\n",
    "    prob = {tag: round(emission_matrix[w][tag] / total, 2) for tag in emission_table.field_names[1:]}\n",
    "    emission_table.add_row([w] + list(prob.values()))\n",
    "print(\"Emission Matrix:\")\n",
    "# print only first 10 rows\n",
    "print(emission_table[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_matrix = defaultdict(lambda: defaultdict(int))\n",
    "previous_tag = None\n",
    "for index, row in df.iterrows():\n",
    "    tag = tag_mapping.get(row['POS'], 'MODAL')\n",
    "    if previous_tag is not None:\n",
    "        transition_matrix[previous_tag][tag] += 1\n",
    "    previous_tag = tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transition Matrix:\n",
      "+-------+------+------+------+------+-------+\n",
      "|       | ADV  | VERB | NOUN | ADJ  | MODAL |\n",
      "+-------+------+------+------+------+-------+\n",
      "|  NOUN | 0.01 | 0.55 | 0.18 | 0.25 |  0.01 |\n",
      "| MODAL | 0.02 | 0.31 | 0.13 | 0.41 |  0.14 |\n",
      "|  VERB | 0.05 | 0.54 | 0.18 | 0.16 |  0.07 |\n",
      "|  ADJ  | 0.0  | 0.13 | 0.01 | 0.77 |  0.09 |\n",
      "|  ADV  | 0.05 | 0.39 | 0.41 | 0.04 |  0.1  |\n",
      "+-------+------+------+------+------+-------+\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTransition Matrix:\")\n",
    "trans_table = PrettyTable()\n",
    "trans_table.field_names = [\"\"] + list(set(tag_mapping.values())) + ['MODAL']\n",
    "for tag in transition_matrix:\n",
    "    total = sum(transition_matrix[tag].values())\n",
    "    prob = {}\n",
    "    for tg in set(tag_mapping.values()) | {'MODAL'}:\n",
    "        prob[tg] = round(transition_matrix[tag][tg] / total, 2)\n",
    "    trans_table.add_row([tag] + list(prob.values()))\n",
    "print(trans_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Predicting POS tags using Viterbi Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viterbi Algorithm\n",
    "def viterbi(ws, emission_matrix, transition_matrix):\n",
    "    tags = list(set(tag_mapping.values())) + ['MODAL']\n",
    "    pi = np.zeros((len(ws), len(tags)))\n",
    "    bp = np.zeros((len(ws), len(tags)), dtype=int)\n",
    "    for i, w in enumerate(ws):\n",
    "        for j, tag in enumerate(tags):\n",
    "            if i == 0:\n",
    "                pi[i][j] = 1\n",
    "            else:\n",
    "                max_prob = -1\n",
    "                max_prob_index = -1\n",
    "                for k, prev_tag in enumerate(tags):\n",
    "                    if emission_matrix[w][tag] == 0:\n",
    "                        emission_matrix[w][tag] = 0.0001\n",
    "                    if transition_matrix[prev_tag][tag] == 0:\n",
    "                        transition_matrix[prev_tag][tag] = 0.0001\n",
    "                    prob = pi[i-1][k] * emission_matrix[w][tag] * transition_matrix[prev_tag][tag]\n",
    "                    if prob > max_prob:\n",
    "                        max_prob = prob\n",
    "                        max_prob_index = k\n",
    "                pi[i][j] = max_prob\n",
    "                bp[i][j] = max_prob_index\n",
    "    max_prob = -1\n",
    "    max_prob_index = -1\n",
    "    for j, tag in enumerate(tags):\n",
    "        if pi[-1][j] > max_prob:\n",
    "            max_prob = pi[-1][j]\n",
    "            max_prob_index = j\n",
    "    predicted_tags = [tags[max_prob_index]]\n",
    "    for i in range(len(ws)-1, 0, -1):\n",
    "        max_prob_index = bp[i][max_prob_index]\n",
    "        predicted_tags.append(tags[max_prob_index])\n",
    "    return list(reversed(predicted_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted tags for the sample sentence:\n",
      "+-----------+-------------------+\n",
      "|    Word   | Predicted POS Tag |\n",
      "+-----------+-------------------+\n",
      "|    The    |       MODAL       |\n",
      "|    sun    |        NOUN       |\n",
      "|   dipped  |        VERB       |\n",
      "|   below   |       MODAL       |\n",
      "|    the    |       MODAL       |\n",
      "|  horizon, |        NOUN       |\n",
      "|  casting  |        VERB       |\n",
      "|     a     |       MODAL       |\n",
      "|   warm,   |       MODAL       |\n",
      "|   golden  |        NOUN       |\n",
      "|    glow   |       MODAL       |\n",
      "|   across  |       MODAL       |\n",
      "|    the    |       MODAL       |\n",
      "| tranquil, |        NOUN       |\n",
      "|  rippling |       MODAL       |\n",
      "|   waters  |        NOUN       |\n",
      "|     of    |       MODAL       |\n",
      "|    the    |       MODAL       |\n",
      "|   lake.   |        NOUN       |\n",
      "+-----------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "sample_sentence = \"The sun dipped below the horizon, casting a warm, golden glow across the tranquil, rippling waters of the lake.\"\n",
    "predicted_tags = viterbi(sample_sentence.split(), emission_matrix, transition_matrix)\n",
    "\n",
    "table = PrettyTable(['Word', 'Predicted POS Tag'])\n",
    "\n",
    "for word, tag in zip(sample_sentence.split(), predicted_tags):\n",
    "    table.add_row([word, tag])\n",
    "\n",
    "print(\"\\nPredicted tags for the sample sentence:\")\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Conclusion\n",
    "In this experiment we learned how to calculate the emission and transition matrix for tagging Parts of Speech. We also learned how to find the POS tags of a given sentence using Viterbi decoding."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bap-notebook",
   "language": "python",
   "name": "bap-notebook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
